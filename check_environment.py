#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼MultiTalkç¯å¢ƒéªŒè¯è„šæœ¬

å¿«é€Ÿæ£€æŸ¥éƒ¨ç½²ç¯å¢ƒæ˜¯å¦å‡†å¤‡å°±ç»ª
"""

import sys
import os
from pathlib import Path
import subprocess

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("ğŸ æ£€æŸ¥Pythonç‰ˆæœ¬...")
    version = sys.version_info
    if version.major == 3 and version.minor >= 10:
        print(f"   âœ… Python {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        print(f"   âŒ Pythonç‰ˆæœ¬è¿‡ä½: {version.major}.{version.minor}.{version.micro}")
        print("   éœ€è¦Python 3.10æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False

def check_cuda():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("ğŸ® æ£€æŸ¥CUDAç¯å¢ƒ...")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"   âœ… æ£€æµ‹åˆ° {gpu_count} å¼ GPU")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory // (1024**3)
                print(f"   GPU {i}: {gpu_name} ({gpu_memory}GB)")
            return True
        else:
            print("   âš ï¸  æœªæ£€æµ‹åˆ°CUDA GPU")
            return False
    except ImportError:
        print("   âŒ PyTorchæœªå®‰è£…")
        return False

def check_dependencies():
    """æ£€æŸ¥å…³é”®ä¾èµ–"""
    print("ğŸ“¦ æ£€æŸ¥å…³é”®ä¾èµ–...")
    
    dependencies = [
        ("torch", "PyTorch"),
        ("torchvision", "TorchVision"),
        ("transformers", "Transformers"),
        ("gradio", "Gradio"),
        ("librosa", "Librosa"),
        ("soundfile", "SoundFile"),
        ("PIL", "Pillow"),
        ("numpy", "NumPy"),
    ]
    
    missing = []
    for module, name in dependencies:
        try:
            __import__(module)
            print(f"   âœ… {name}")
        except ImportError:
            print(f"   âŒ {name}")
            missing.append(name)
    
    if missing:
        print(f"\n   ç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print("   è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    return True

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    print("ğŸ§  æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    
    required_paths = [
        ("weights/Wan2.1-I2V-14B-480P", "ä¸»æ¨¡å‹"),
        ("weights/chinese-wav2vec2-base", "éŸ³é¢‘ç¼–ç å™¨"),
        ("weights/Kokoro-82M", "TTSæ¨¡å‹"),
        ("weights/Kokoro-82M/voices", "è¯­éŸ³æ¨¡å‹"),
    ]
    
    missing = []
    for path, name in required_paths:
        if Path(path).exists():
            print(f"   âœ… {name}: {path}")
        else:
            print(f"   âŒ {name}: {path}")
            missing.append(name)
    
    # æ£€æŸ¥å…·ä½“è¯­éŸ³æ–‡ä»¶
    voice_files = [
        "weights/Kokoro-82M/voices/af_heart.pt",
        "weights/Kokoro-82M/voices/am_adam.pt", 
        "weights/Kokoro-82M/voices/af_bella.pt",
        "weights/Kokoro-82M/voices/am_freeman.pt",
    ]
    
    for voice_file in voice_files:
        if Path(voice_file).exists():
            print(f"   âœ… è¯­éŸ³æ¨¡å‹: {Path(voice_file).name}")
        else:
            print(f"   âŒ è¯­éŸ³æ¨¡å‹: {Path(voice_file).name}")
            missing.append(f"è¯­éŸ³æ¨¡å‹ {Path(voice_file).name}")
    
    return len(missing) == 0

def check_service_files():
    """æ£€æŸ¥æœåŠ¡æ–‡ä»¶"""
    print("ğŸ“ æ£€æŸ¥æœåŠ¡æ–‡ä»¶...")
    
    required_files = [
        "distributed_multitalk_app.py",
        "distributed_generator.py",
        "distributed_web_interface.py", 
        "distributed_multitalk_core.py",
        "start_distributed_service.bat",
        "start_distributed_service.sh",
    ]
    
    missing = []
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"   âœ… {file_path}")
        else:
            print(f"   âŒ {file_path}")
            missing.append(file_path)
    
    return len(missing) == 0

def check_ports():
    """æ£€æŸ¥ç«¯å£å ç”¨"""
    print("ğŸ”Œ æ£€æŸ¥ç«¯å£çŠ¶æ€...")
    
    try:
        import socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', 8419))
        sock.close()
        
        if result == 0:
            print("   âš ï¸  ç«¯å£8419å·²è¢«å ç”¨")
            return False
        else:
            print("   âœ… ç«¯å£8419å¯ç”¨")
            return True
    except Exception:
        print("   âš ï¸  æ— æ³•æ£€æŸ¥ç«¯å£çŠ¶æ€")
        return True

def check_disk_space():
    """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
    print("ğŸ’¾ æ£€æŸ¥ç£ç›˜ç©ºé—´...")
    
    try:
        import shutil
        free_space = shutil.disk_usage('.').free // (1024**3)
        
        if free_space >= 50:
            print(f"   âœ… å¯ç”¨ç©ºé—´: {free_space}GB")
            return True
        else:
            print(f"   âš ï¸  å¯ç”¨ç©ºé—´ä¸è¶³: {free_space}GB (å»ºè®®è‡³å°‘50GB)")
            return False
    except Exception:
        print("   âš ï¸  æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´")
        return True

def suggest_startup_command():
    """å»ºè®®å¯åŠ¨å‘½ä»¤"""
    print("\nğŸš€ å»ºè®®çš„å¯åŠ¨å‘½ä»¤:")
    print("=" * 50)
    
    try:
        import torch
        gpu_count = torch.cuda.device_count()
        
        if gpu_count >= 8:
            print("8GPUåˆ†å¸ƒå¼æ¨¡å¼ï¼ˆæ¨èï¼‰:")
            print("Windows: start_distributed_service.bat distributed")
            print("Linux:   bash start_distributed_service.sh distributed")
            print("\næ‰‹åŠ¨å¯åŠ¨:")
            print("torchrun --nproc_per_node=8 --master_port=29500 \\")
            print("    distributed_multitalk_app.py \\")
            print("    --ulysses_size=8 --ring_size=1 \\")
            print("    --t5_fsdp --dit_fsdp --server_port=8419")
        else:
            print("å•GPUæµ‹è¯•æ¨¡å¼:")
            print("Windows: start_distributed_service.bat single")
            print("Linux:   bash start_distributed_service.sh single")
            print("\næ‰‹åŠ¨å¯åŠ¨:")
            print("python distributed_multitalk_app.py \\")
            print("    --ulysses_size=1 --ring_size=1 --server_port=8419")
    except:
        print("è¯·å…ˆå®‰è£…PyTorchåé‡æ–°æ£€æŸ¥")

def main():
    """ä¸»æ£€æŸ¥å‡½æ•°"""
    print("=" * 60)
    print("ğŸ” åˆ†å¸ƒå¼MultiTalkç¯å¢ƒéªŒè¯")
    print("=" * 60)
    
    checks = [
        ("Pythonç‰ˆæœ¬", check_python_version),
        ("CUDAç¯å¢ƒ", check_cuda),
        ("ä¾èµ–åŒ…", check_dependencies),
        ("æ¨¡å‹æ–‡ä»¶", check_model_files),
        ("æœåŠ¡æ–‡ä»¶", check_service_files),
        ("ç«¯å£çŠ¶æ€", check_ports),
        ("ç£ç›˜ç©ºé—´", check_disk_space),
    ]
    
    passed = 0
    total = len(checks)
    
    for check_name, check_func in checks:
        print(f"\nğŸ“‹ {check_name}:")
        print("-" * 30)
        
        if check_func():
            passed += 1
            
    print("\n" + "=" * 60)
    print(f"ğŸ¯ æ£€æŸ¥ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ ç¯å¢ƒæ£€æŸ¥å…¨éƒ¨é€šè¿‡ï¼å¯ä»¥å¯åŠ¨æœåŠ¡ã€‚")
        suggest_startup_command()
        print("\nğŸ“± å¯åŠ¨åè®¿é—®: http://localhost:8419")
    elif passed >= total - 2:
        print("âš ï¸  å¤§éƒ¨åˆ†æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å°è¯•å¯åŠ¨æœåŠ¡ã€‚")
        suggest_startup_command()
    else:
        print("âŒ ç¯å¢ƒæ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·è§£å†³ç›¸å…³é—®é¢˜åé‡è¯•ã€‚")
        
        print("\nğŸ”§ å¸¸è§è§£å†³æ–¹æ¡ˆ:")
        print("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
        print("2. ä¸‹è½½æ¨¡å‹: è¯·å‚è€ƒä¸»é¡¹ç›®README")
        print("3. æ£€æŸ¥CUDA: nvidia-smi")
        print("4. é‡Šæ”¾ç«¯å£: å…³é—­å ç”¨8419ç«¯å£çš„ç¨‹åº")
    
    print("=" * 60)

if __name__ == "__main__":
    main()